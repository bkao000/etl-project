{\rtf1\ansi\ansicpg1252\cocoartf2709
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Arial-BoldMT;\f1\froman\fcharset0 Times-Roman;\f2\fswiss\fcharset0 ArialMT;
\f3\fswiss\fcharset0 Arial-ItalicMT;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red255\green255\blue255;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;\cssrgb\c100000\c100000\c100000;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{none\}}{\leveltext\leveltemplateid1\'00;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{none\}}{\leveltext\leveltemplateid101\'00;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid2}
{\list\listtemplateid3\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{none\}}{\leveltext\leveltemplateid201\'00;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid3}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}{\listoverride\listid3\listoverridecount0\ls3}}
\margl1440\margr1440\vieww29300\viewh17140\viewkind0
\deftab720
\pard\pardeftab720\sa240\partightenfactor0

\f0\b\fs45\fsmilli22667 \cf2 \cb3 \expnd0\expndtw0\kerning0
	Requirements 
\f1\b0\fs24 \cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa240\partightenfactor0
\ls1\ilvl0
\f2\fs29\fsmilli14667 \cf2 \cb3 \kerning1\expnd0\expndtw0 		\expnd0\expndtw0\kerning0
\uc0\u9679  \'a0The transformations should be configurable with an external DSL (like a configuration file) 
\f1\fs24 \cb1 \uc0\u8232 	=> Source fields, target fields and source to target mappings are defined in a yaml file - \'93order.yaml\'94.\
\ls1\ilvl0
\f2\fs29\fsmilli14667 \cb3 \kerning1\expnd0\expndtw0 		\expnd0\expndtw0\kerning0
\uc0\u9679  \'a0The functionality should be implemented as a library, without (significant) external dependencies 
\f1\fs24 \cb1 \uc0\u8232 	=> \'93etc-lib\'94 library is created as a package. It contains the etl transformation functions.\
\ls1\ilvl0
\f2\fs29\fsmilli14667 \cb3 \kerning1\expnd0\expndtw0 		\expnd0\expndtw0\kerning0
\uc0\u9679  \'a0Invalid rows should be collected, with errors describing why they are invalid (logging them is fine for now) 
\f1\fs24 \cb1 \uc0\u8232 	=> Invalid rows are collected in an error file - \'93err-order-output.csv\'94 with error description, error field, error row data. \
\ls1\ilvl0
\f2\fs29\fsmilli14667 \cb3 \kerning1\expnd0\expndtw0 		\expnd0\expndtw0\kerning0
\uc0\u9679  \'a0The data tables can have a very large number of rows 
\f1\fs24 \cb1 \uc0\u8232 	=> If source row volume is big consider to break it into smaller chunks and to process it more frequently. Rely on one task to process a big volume of data has some drawbacks as below.\uc0\u8232 		- The task might hit the resource limit as data grows over time,\u8232 		- If the job fails it will take a long time to rerun it.\u8232 		- Big volume of source data and target data could make the troubleshooting cumbersome.
\f0\b\fs45\fsmilli22667 \cb3 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa240\partightenfactor0
\ls2\ilvl0\cf2 \kerning1\expnd0\expndtw0 		\expnd0\expndtw0\kerning0
Deliverables 
\f1\b0\fs24 \cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sa240\partightenfactor0
\ls3\ilvl0
\f2\fs29\fsmilli14667 \cf2 \cb3 \kerning1\expnd0\expndtw0 		\expnd0\expndtw0\kerning0
\uc0\u9679  \'a0Running code and test suite provided through online code repo or in a tar-ball\u8232 
\fs24 	=> tar-ball file name: etl-project.tgz\uc0\u8232 	=> GitHub repository: https://github.com/bkao000/etl-project\u8232 	=> One application etl-app and one library etl-lib are included in the delivery with tests suite and test cases.
\f1 \cb1 \
\ls3\ilvl0
\f2\fs29\fsmilli14667 \cb3 \kerning1\expnd0\expndtw0 		\expnd0\expndtw0\kerning0
\uc0\u9679  \'a0Instructions on how to build and run the code with example data 
\f1\fs24 \cb1 \uc0\u8232 	=>	1. Uncompress the tar ball or the downloaded project zip from GitHub URL provided.\u8232 		2. Set up environment variable ETL_PROJECT to the path of project folder (eta-project).\u8232 		3. See \'93README.md\'94 file in \'93$\{ETL_PROJECT\}/etl-app\'94 folder.\
\ls3\ilvl0
\f2\fs29\fsmilli14667 \cb3 \kerning1\expnd0\expndtw0 		\expnd0\expndtw0\kerning0
\uc0\u9679  \'a0
\f3\i Short 
\f2\i0 architectural overview and technology choices made 
\f1\fs24 \cb1 \uc0\u8232 	=> See \'93etl-project-design.pdf\'94 in $\{ETL_PROJECT\}/etl-app/doc\
\ls3\ilvl0
\f2\fs29\fsmilli14667 \cb3 \kerning1\expnd0\expndtw0 		\expnd0\expndtw0\kerning0
\uc0\u9679  \'a0(Basic) documentation, unless it\'92s completely self-documenting (to a fellow software 
\f1\fs24 \cb1 \uc0\u8232 
\f2\fs29\fsmilli14667 \cb3 developer) 
\f1\fs24 \cb1 \uc0\u8232 	=> Comments in code.\u8232 	=> Pipeline design in etl-project-design.doc.\
\ls3\ilvl0
\f2\fs29\fsmilli14667 \cb3 \kerning1\expnd0\expndtw0 		\expnd0\expndtw0\kerning0
\uc0\u9679  \'a0List of assumptions or simplifications made 
\f1\fs24 \cb1 \uc0\u8232 	=> A row is considered as good when all the target fields can be published based on the definitions in yaml file. Other unused source fields contents will not be checked.\u8232 	=> A row precess will stop when the first field transformation error is hit. The rest of fields will not be checked.\u8232 	=> If a target field should be set up with a default value but it is not defined in the yaml file, the whole job will stop when this error is hit for the first time.\u8232 	=> Each target field in yaml file should contain required attributes: src, tgt, type.\u8232 	=> Source formats of numeric data are not checked. Rely on conversion functions to capture errors.\u8232 	=> A test yaml file \'93order-product-name-no-src-format-check.yaml\'94 is created for the following scenario. \uc0\u8232 	     Since the source data contains leading lowercase character in \'93Product Name\'94 I assume the data doesn\'92t compliant with the source format definition \'93[A-Z]+\'94. Instead of filtering out the invalid source data I transform the data to uppercase to meet the requirement in target.\
\ls3\ilvl0
\f2\fs29\fsmilli14667 \cb3 \kerning1\expnd0\expndtw0 		\expnd0\expndtw0\kerning0
\uc0\u9679  \'a0List of the next steps you would want to do if this were a real project\u8232 	
\f1\fs24 \cb1 => Convert \'93etl_order.py\'94 to a framework to handle all similar type of data sets by taking table name/data set name such as \'91order\'92 or \'91user\'92 as a parameter. This parameter can be used to locate the source file, configuration file, output file and error log file in the predefined folder structure.\uc0\u8232 	=> Config jobs to run in a scheduler such as Airflow. Set up dependencies between a job and its upstream and/or downstream. Set up job monitors and alerts.\u8232 	=> Add data quality checks and alerts.\u8232 	=> Create a dashboard/report to show daily production jobs/pipelines and data quality status.\u8232 	=> Write a tool to review and validate yaml file to ensure it is in the correct format and contains all required definitions such as src field, target field, target type.\u8232 	=> Create a configure file or table to register ETL pipelines. Scheduler can pick up jobs to run from the registry.\u8232 	=> Add dependency check to ETL pipelines to ensure each stage of a pipeline starts with good upstream data.\
\pard\tx720\pardeftab720\sa240\partightenfactor0
\cf2 \
\pard\tx720\pardeftab720\sa240\partightenfactor0
\cf2 \
}